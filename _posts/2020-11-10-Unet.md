---
layout: post
title: Tutorial - Unet com Tensorflow 2.0 
subtitle:  Códigos para montar a arquitetura de rede Unet
tags: [redes,Unet,teoria]
---

Neste post mostrarei como gerar a arquitetura de rede Unet utilizando a biblioteca Tensorflow 2.0. Esta arquitetura é muito utilizada 
em problemas de segmentação semântica. Sua estrutura é composta por uma parte contracional, também conhecida como encoder, e uma parte 
extensional (decoder).


![](/img/unet.jpeg)


A parte encoder apresenta duas camadas de convolução 2D ativadas com a função ReLU, seguidas de uma camada de max-pooling 2D para reduzir a dimensão
dos canais de atributos (feature channels) pela metade. Apesar de no artigo original da Unet não existirem camadas de Dropout, em nossa rede também
adicionaremos uma destas camadas após a camada de max-pooling. Assim, primeiro iremos definir a função (conv_block) que criará as duas camadas de convolução ativadas
com a camada ReLU.

A função conv_block apresenta 3 parâmentros: input_tensor que indica a dimensão do tensor de entrada, n_filters que indica a quantidade de filtros e kernel_size
que indica o tamanho do kernel. A função retorna o output dos blocos convolucionais.

``` python
import tensorflow as tf

def conv_block(input_tensor, n_filters, kernel_size=3):
    """
    Adiciona 2 blocos convolucionais ativados com a função ReLU
    :param input_tensor: (tuple) dimensão do tensor de input.
    :param n_filters: (int) quantidade de filtros.
    :param kernel_size: tamanho do kernel
    :return: 2 camadas de convolução ativadas com a função ReLU.
    """

    inputs = input_tensor

    for i in range(2):
        x = tf.keras.layers.Conv2D(filters=n_filters, kernel_size=kernel_size,
                                   kernel_initializer="he_normal", padding="same")(inputs)
        x = tf.keras.layers.Activation("relu")(x)

        return x

```

Agora iremos criar a função encoder_block para adicionar as camadas de max-pooling e dropout. Esta função tem como parâmetro inputs, n_filters, pool_size e
dropout. O inputs representa o tensor de entrada; n_filters indica a quatidade de filtros; pool_size indica o tamanho do filtro de pooling; e dropout indica
a probabilidade de desativação dos layers. Note que essa função retorna dois valores, um da etapa de convolução e outro referente ao resultado da operação
de max-pooling. Isto ocorre pois os resultados da operação de convolução serão utilizados posteriormente na etapa decoder.


``` python
def encoder_block(inputs, n_filters=64, pool_size=(2,2), dropout=0.3):
    """
    Criar o bloco encoder com duas camadas de convolução, uma camada de maxpooling e uma de dropout.

    :param inputs: (tf.tensor) tensor de entrada.
    :param n_filters: (int) quantidade de filtros. default = 64
    :param pool_size:  (tuple) dimensão do pooling. default = (2,2)
    :param dropout:  (float) probabilidade do dropout. default = 0.3
    :return:
    conv (tf.tensor) resultado da convolução;
    p (tf.tensor) resultado da operação de pooling e dropout.
    """
    # duas camadas de convolução
    conv = conv_block(inputs, n_filters=n_filters)
    # camada de max pooling
    p = tf.keras.layers.MaxPooling2D(pool_size=pool_size)(conv)
    # camada de dropout
    p = tf.keras.layers.Dropout(dropout)(p)

    return conv, p

```

Agora já temos os nossos blocos convolucinais definidos, portanto, podemos montar a parte encoder da rede Unet. Assim a função encoder criará 4 blocos convolucionais.
Esta função recebe o tensor como input e retorna 5 valores. O primeiro valor corresponde ao output do último bloco encoder. Já os outros valores correspondem
ao output de cada bloco.


``` python
def encoder(inputs):
    """

    :param inputs: (tf.tensor) tensor de etrada.
    :return:
    p4 (tf.tensor) output do maxpooling do último bloco de encoder.
    (f1,f2,f3,f4) (tf.tensor) output the cada bloco de encoder.
    """

    f1, p1 = encoder_block(inputs, n_filters=64, pool_size=(2,2),dropout=0.3)
    f2, p2 = encoder_block(p1, n_filters=128, pool_size=(2, 2), dropout=0.3)
    f3, p3 = encoder_block(p2, n_filters=256, pool_size=(2, 2), dropout=0.3)
    f4, p4 = encoder_block(p3, n_filters=512, pool_size=(2, 2), dropout=0.3)

    return p4, (f1, f2, f3, f4)

```

Antes da etapa de expansão (decoder) existem mais duas operação de convolução, isto caracteriza o gargalo (bottleneck) da rede. Assim, iremos definir a função
bottleneck para realizar esta operação. Esta função utilizará a função conv_block (definida anteriormente) para gerar estas duas operações de convolução. O input 
desta função é um tensor e como output ela retorna o resultado das duas operações de convolução.


``` python
def bottleneck(inputs):
    """
    :param inputs: (tf.tensor) tensor de entrada.
    :return: (tf.tensor) saída do bottleneck.
    """

    bottleneck = conv_block(inputs, n_filters=1024)

    return bottleneck

```

Agora iremos construir a parte extensional (decoder) da rede. Os blocos de extensão consistem de uma operação de reamostragem (upsampling) que irá aumentar
a dimensão do tensor de entrada em 2 vezes. Utilizaremos a operação de convolução transposta 2D (explicarei em um post futuro como esta operação funciona) para isto.
Em seguida, uma camada de concatenação irá juntar o resultado desta operação ao tensor de dimensão equivalente obtido na etapa encoder. O resultado da operação
de concatenação irá para uma camada de dropout e posteriormente passará por duas camadas de convolução (novamente utilizaremos a função conv_block para isto).

A função decoder_block tem 6 parâmentros. Cada um destes está descrito abaixo da função. A função retornará um tensor de dimensão duas vezes maior em relação
ao tensor de entrada.


``` python
def decoder_block(inputs, conv_output, n_filters=64, kernel_size=3, strides=3, dropout=0.3):
    """
    Bloco decoder da rede Unet.

    :param inputs: (tf.tensor) tensor de entrada.
    :param conv_output: (tf.tensor) tensor obtido no caminho encoder que será concatenado.
    :param n_filters: (int) número de filtros.
    :param kernel_size: (int) dimensão do kernel.
    :param strides: (int) deslocamento lateral dos filtros.
    :param dropout: (float)  probabilidade de desativação dos filtros.
    :return:
    concat (tf.tensor) resultado do bloco de convolução
    """

    up = tf.keras.layers.Conv2DTranspose(n_filters, kernel_size, strides=strides, padding="same")(inputs)
    concat = tf.keras.layers.Concatenate()([up, conv_output])
    concat = tf.keras.layers.Dropout(dropout)(concat)
    concat = conv_block(concat,n_filters,kernel_size=3)

    return concat

```
Agora iremos montar a etapa decoder da rede Unet. Para isto defineremos a função decoder (abaixo).  


``` python
def decoder(inputs, convs, output_channels, activation="sigmoid"):
    """
    Etapa decoder Unet

    :param inputs: (tf.tensor) tensor de entrada
    :param convs: (tuple(tf.tensor)) tupla contendo os tensores da etapa encoder
    :param output_channels: (int) quantidade de canais de saida
    :param activation: (string) fução de ativação de saída. default = sigmoid
    :return:
    """

    f1, f2, f3, f4 = convs

    c6 = decoder_block(inputs, f4, n_filters=512, kernel_size=3, strides=2, dropout=0.3)
    c7 = decoder_block(c6, f3, n_filters=256, kernel_size=3, strides=2, dropout=0.3)
    c8 = decoder_block(c7, f2, n_filters=128, kernel_size=3, strides=2, dropout=0.3)
    c9 = decoder_block(c8, f1, n_filters=64, kernel_size=3, strides=2, dropout=0.3)

    outputs = tf.keras.layers.Conv2D(output_channels, (1,1), activation=activation) (c9)

    return outputs
```
Agora para finalizar esta arquitetura iremos definir a função unet. Está função irá montar a rede Unet (similar a da primeira imagem).

``` python
def unet(input_shape = (128,128,3), output_channels=1, activation="sigmoid"):
    """

    :param input_shape: (tuple) dimensção do layer de entrada. default = (128,128,3).
    :param output_channels:(int) Número de canais de output
    :param activation: (string) função de ativação.
    :return: (tf.keras.models.Model) arquitetura de rede Unet.
    """

    # definir a dimensão da camada de input
    inputs = tf.keras.layers.Input(shape=input_shape)

    # bloco encoder
    encoder_output, convs = encoder(inputs)

    # bottleneck
    bottle_neck = bottleneck(encoder_output)

    # decoder
    outputs = decoder(bottle_neck,convs, output_channels=output_channels,activation=activation)

    # montar o modelo
    model = tf.keras.Model(inputs, outputs)

    return model
```

Agora basta associarmos o modelo a uma variável para obtermos o resumo da rede utilizando o método .summary().

``` python
modelo = unet()
modelo.summary()
```

![](/img/summary.png)
